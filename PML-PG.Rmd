---
title: "PML-PG"
output: html_document
---


<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<title>Peer Graded Assignment for Practical Machine Learning Course</title>

</head>
<style>
h1
{background-color: Lime;
color: Chocolate;}
h2
{background-color: green;
color: yellow;}
h3
{background-color: maroon;
color: SpringGreen;}
p
{background-color: DarkSlateBlue;
color: DarkOrange;}
body
{background-color: DarkSalmon;}
</style>

<body>



<div id="header">
<h1 class="title">Peer Graded Assignment for Course- Practical Machine Learning</h1>
<h6>Nishant Sirohi</h6>
<h3 class="date"><em>Sunday, October 18, 2020</em></h3>
</div>


<div id="summary" class="section level2">
<h2>Summary of the Project</h2>
<p>This report makes use of Machine Learning algorithms to analyse the manner in which the devices affect the way that a user does exercise.</p>
<div id="background" class="section level3">
<h2>Background</h2>
<p>Using devices such as Jawbone Up, Nike Fuel Band, and Fit-bit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website <a href="http://groupware.les.inf.puc-rio.br/har">here:</a> (see the section on the Weight Lifting Exercise Dataset).</p>
</div>
<div id="data" class="section level3">
<h2>Data</h2>
<p>The training data for this project are available here:</p>
<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" class="uri">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>
<p>The test data are available here:</p>
<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" class="uri">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>
</div>
<div id="set-the-work-environment-and-knitr-options" class="section level3">
<h2>Here we set the working environment and KnitR</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>(<span class="dt">all=</span><span class="ot">TRUE</span>)) <span class="co">#start with empty workspace</span>
startTime &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()

<span class="kw">library</span>(knitr)
opts_chunk$<span class="kw">set</span>(<span class="dt">echo =</span> <span class="ot">TRUE</span>, <span class="dt">cache=</span> <span class="ot">TRUE</span>, <span class="dt">results =</span> <span class="st">'hold'</span>)</code></pre>
</div>
<div id="load-libraries-and-set-seed" class="section level3">
<h2>Loading of libraries and setting of seeds</h2>
<p>Loading all of the libraries used, and setting seeds for the purpose of reproducibility.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ElemStatLearn)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(RCurl)
<span class="kw">set.seed</span>(<span class="dv">2014</span>)</code></pre>
</div>
<div id="load-and-prepare-the-data-and-clean-up-the-data" class="section level3">
<h2>Loading and preparing the data to be used and cleaning it up.</h2>
<p>Loading and preparing the data</p>
<pre class="sourceCode r"><code class="sourceCode r">trainingLink &lt;-<span class="st"> </span><span class="kw">getURL</span>(<span class="st">&quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;</span>)
pml_CSV  &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">text =</span> trainingLink, <span class="dt">header=</span><span class="ot">TRUE</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>, <span class="dt">na.strings=</span><span class="kw">c</span>(<span class="st">&quot;NA&quot;</span>,<span class="st">&quot;&quot;</span>))

pml_CSV &lt;-<span class="st"> </span>pml_CSV[,-<span class="dv">1</span>] <span class="co"># Remove the first column that represents a ID Row</span></code></pre>
</div>
<div id="data-sets-partitions-definitions" class="section level3">
<h2>Data Sets Partitions Definitions</h2>
<p>Creation of data partitions of training and validation of datasets.</p>
<pre class="sourceCode r"><code class="sourceCode r">inTrain =<span class="st"> </span><span class="kw">createDataPartition</span>(pml_CSV$classe, <span class="dt">p=</span><span class="fl">0.60</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)
training =<span class="st"> </span>pml_CSV[inTrain,]
validating =<span class="st"> </span>pml_CSV[-inTrain,]

<span class="co"># number of rows and columns of data in the training set</span>

<span class="kw">dim</span>(training)

<span class="co"># number of rows and columns of data in the validating set</span>

<span class="kw">dim</span>(validating)</code></pre>
<pre><code>## [1] 11776   159
## [1] 7846  159</code></pre>
</div>
</div>
<div id="data-exploration-and-cleaning" class="section level2">
<h2>Data Exploration and Cleaning of data</h2>
<p>As we can see that the data we have has too many columns and we have chosen a random forest model, first we check if we have problems in columns without data. We remove the columns having less than 60% of data entry.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of columns having  less than 60% of data entry</span>
<span class="kw">sum</span>((<span class="kw">colSums</span>(!<span class="kw">is.na</span>(training[,-<span class="kw">ncol</span>(training)])) &lt;<span class="st"> </span><span class="fl">0.6</span>*<span class="kw">nrow</span>(training)))</code></pre>
<p>[1] 100</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># applying our definition of removing columns which doesn't have an amount of data, before it is applied to the actual model.</span>

Keep &lt;-<span class="st"> </span><span class="kw">c</span>((<span class="kw">colSums</span>(!<span class="kw">is.na</span>(training[,-<span class="kw">ncol</span>(training)])) &gt;=<span class="st"> </span><span class="fl">0.6</span>*<span class="kw">nrow</span>(training)))
training   &lt;-<span class="st">  </span>training[,Keep]
validating &lt;-<span class="st"> </span>validating[,Keep]

<span class="co"># number of columns and the number of rows in the final training data set</span>

<span class="kw">dim</span>(training)</code></pre>
<p>[1] 11776 59</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># number of columns and rows in the final data set to be used for analysis</span>

<span class="kw">dim</span>(validating)</code></pre>
<p>[1] 7846 59</p>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the execution. So, we proceed with the training the model (Random Forest) with the training data set.</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(classe~.,<span class="dt">data=</span>training)
<span class="kw">print</span>(model)</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = classe ~ ., data = training) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.19%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3348    0    0    0    0 0.000000000
## B    3 2276    0    0    0 0.001316367
## C    0    9 2044    1    0 0.004868549
## D    0    0    5 1924    1 0.003108808
## E    0    0    0    3 2162 0.001385681</code></pre>
<div id="model-evaluate" class="section level3">
<h3>Model Evaluate</h3>
<p>And proceed with the verification of variable importance measures as produced by random Forest:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">importance</span>(model)</code></pre>
<pre><code>##                      MeanDecreaseGini
## user_name                  90.3730521
## raw_timestamp_part_1      932.6373613
## raw_timestamp_part_2       11.2278639
## cvtd_timestamp           1417.2774858
## new_window                  0.2375014
## num_window                538.0882905
## roll_belt                 547.0536679
## pitch_belt                289.9572495
## yaw_belt                  342.2318442
## total_accel_belt          110.0633448
## gyros_belt_x               39.5889951
## gyros_belt_y               45.1635594
## gyros_belt_z              116.7332576
## accel_belt_x               65.1914831
## accel_belt_y               71.6575192
## accel_belt_z              174.5775863
## magnet_belt_x             109.5946175
## magnet_belt_y             198.2364446
## magnet_belt_z             174.1100246
## roll_arm                  123.8385402
## pitch_arm                  56.7411710
## yaw_arm                    80.6033881
## total_accel_arm            26.3682367
## gyros_arm_x                42.2808067
## gyros_arm_y                41.6757042
## gyros_arm_z                19.4557642
## accel_arm_x                94.6270535
## accel_arm_y                54.4922538
## accel_arm_z                40.7576689
## magnet_arm_x              105.2342845
## magnet_arm_y               79.6373607
## magnet_arm_z               57.7204415
## roll_dumbbell             197.6213608
## pitch_dumbbell             75.0525013
## yaw_dumbbell              104.9213658
## total_accel_dumbbell      112.5343776
## gyros_dumbbell_x           42.7839013
## gyros_dumbbell_y          110.7356305
## gyros_dumbbell_z           25.1911639
## accel_dumbbell_x          126.2760046
## accel_dumbbell_y          183.1386045
## accel_dumbbell_z          140.3221880
## magnet_dumbbell_x         234.3036947
## magnet_dumbbell_y         321.8106105
## magnet_dumbbell_z         299.7706537
## roll_forearm              232.9445408
## pitch_forearm             293.0121796
## yaw_forearm                59.1226542
## total_accel_forearm        33.2545324
## gyros_forearm_x            24.9673052
## gyros_forearm_y            41.4192787
## gyros_forearm_z            26.9075827
## accel_forearm_x           133.6714294
## accel_forearm_y            45.3258310
## accel_forearm_z            96.1075329
## magnet_forearm_x           76.6923241
## magnet_forearm_y           76.9926445
## magnet_forearm_z           97.9443069</code></pre>
<p>Now we evaluate our model results through confusion Matrix.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(model,<span class="dt">newdata=</span>validating[,-<span class="kw">ncol</span>(validating)]),validating$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2231    0    0    0    0
##          B    1 1518    5    0    0
##          C    0    0 1362    1    0
##          D    0    0    1 1285    1
##          E    0    0    0    0 1441
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9989          
##                  95% CI : (0.9978, 0.9995)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9985          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9996   1.0000   0.9956   0.9992   0.9993
## Specificity            1.0000   0.9991   0.9998   0.9997   1.0000
## Pos Pred Value         1.0000   0.9961   0.9993   0.9984   1.0000
## Neg Pred Value         0.9998   1.0000   0.9991   0.9998   0.9998
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2843   0.1935   0.1736   0.1638   0.1837
## Detection Prevalence   0.2843   0.1942   0.1737   0.1640   0.1837
## Balanced Accuracy      0.9998   0.9995   0.9977   0.9995   0.9997</code></pre>
<p>And confirmed the accuracy at validating data set by calculate it with the formula:</p>
<pre class="sourceCode r"><code class="sourceCode r">accuracy &lt;-<span class="kw">c</span>(<span class="kw">as.numeric</span>(<span class="kw">predict</span>(model,<span class="dt">newdata=</span>validating[,-<span class="kw">ncol</span>(validating)])==validating$classe))

accuracy &lt;-<span class="kw">sum</span>(accuracy)*<span class="dv">100</span>/<span class="kw">nrow</span>(validating)</code></pre>
<p>Model Accuracy as tested over Validation set = <strong>99.9%</strong>.</p>
</div>
<div id="model-test" class="section level3">
<h3>Model Test</h3>
<p>Finally, we proceed with predicting the new values in the testing csv provided, first we apply the same data cleaning operations on it and coerce all columns of testing data set for the same class of previous data set.</p>
<div id="getting-testing-dataset" class="section level4">
<h4>Getting Testing Dataset</h4>
<pre class="sourceCode r"><code class="sourceCode r">testingLink &lt;-<span class="st"> </span><span class="kw">getURL</span>(<span class="st">&quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;</span>)
pml_CSV  &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">text =</span> testingLink, <span class="dt">header=</span><span class="ot">TRUE</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>, <span class="dt">na.strings=</span><span class="kw">c</span>(<span class="st">&quot;NA&quot;</span>,<span class="st">&quot;&quot;</span>))

pml_CSV &lt;-<span class="st"> </span>pml_CSV[,-<span class="dv">1</span>] <span class="co"># Remove the first column that represents a ID Row</span>
pml_CSV &lt;-<span class="st"> </span>pml_CSV[ , Keep] <span class="co"># Keep the same columns of testing dataset</span>
pml_CSV &lt;-<span class="st"> </span>pml_CSV[,-<span class="kw">ncol</span>(pml_CSV)] <span class="co"># Remove the problem ID</span>

<span class="co"># Apply the Same Transformations and Coerce Testing Dataset</span>

<span class="co"># Coerce testing dataset to same class and strucuture of training dataset </span>
testing &lt;-<span class="st"> </span><span class="kw">rbind</span>(training[<span class="dv">100</span>, -<span class="dv">59</span>] , pml_CSV) 
<span class="co"># Apply the ID Row to row.names and 100 for dummy row from testing dataset </span>
<span class="kw">row.names</span>(testing) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">1</span>:<span class="dv">20</span>)</code></pre>
</div>
<div id="predicting-with-testing-dataset" class="section level4">
<h4>Predicting with testing dataset</h4>
<pre class="sourceCode r"><code class="sourceCode r">predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(model,<span class="dt">newdata=</span>testing[-<span class="dv">1</span>,])
<span class="kw">print</span>(predictions)</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E</code></pre>
</div>




</div>

</div>
</div>


</div>
<p>The analysis was completed on Sat Oct 18, 2020.</p>



</body>
</html>